# ETL Pipeline with Multiple Storage Systems

A comprehensive ETL (Extract-Transform-Load) pipeline demonstrating data processing across different database paradigms: relational (PostgreSQL), document-oriented (MongoDB), and in-memory cache (Redis). Includes performance benchmarking of database schemas (3NF, Star, Snowflake) and file formats (CSV, JSON, Avro, Parquet).


## ğŸ¯ Overview

This project implements a complete ETL pipeline that:
1. **Extracts** data from the DummyJSON API (products, users, orders)
2. **Transforms** data into normalized (3NF) format
3. **Loads** data into three different storage systems:
   - PostgreSQL (3NF, Star Schema, Snowflake Schema)
   - MongoDB (denormalized documents)
   - Redis (caching layer)

The project includes comprehensive benchmarks comparing:
- Insert/read performance across databases
- File format efficiency (CSV, JSON, Avro, Parquet)
- Query performance across different schema designs

## âœ¨ Features

### ETL Pipeline
- âœ… Data extraction from REST API
- âœ… Normalization to Third Normal Form (3NF)
- âœ… Multi-target loading (PostgreSQL, MongoDB, Redis)
- âœ… Automatic data type conversion and validation

### Database Schemas
- âœ… **3NF Schema** â€” Fully normalized relational model
- âœ… **Star Schema** â€” Dimensional model for analytics
- âœ… **Snowflake Schema** â€” Normalized dimensional model
- âœ… Auto-generated date dimension (2020-2026)

### File Format Support
- âœ… CSV reading/writing with type inference
- âœ… JSON with streaming support (JSONL)
- âœ… Avro with schema auto-generation
- âœ… Parquet with multiple compression options

### Benchmarking
- âœ… Insert performance (single vs batch)
- âœ… Read performance comparison
- âœ… Query performance across schemas
- âœ… File format size and speed analysis

### Bonus Features
- âœ… Docker Compose setup with pgAdmin & Redis Commander
- âœ… Fact table export to Parquet/JSON
- âœ… Comprehensive validation tests
- âœ… Performance monitoring and logging

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DummyJSON   â”‚
â”‚     API     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Extract
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transform  â”‚
â”‚    (3NF)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Load
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚  â”‚ MongoDB  â”‚  â”‚  Redis  â”‚
â”‚  - 3NF      â”‚  â”‚ Documentsâ”‚  â”‚  Cache  â”‚
â”‚  - Star     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  - Snowflakeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Prerequisites

- **Python 3.11+**
- **Docker & Docker Compose** (recommended)
- **Git**

### Without Docker:
- PostgreSQL 15+
- MongoDB 7.0+
- Redis 7.0+

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/k-zozulia/data_hw_2.git
cd data_hw_2
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Start Database Services

### Option A: Using Docker (Recommended)

```bash
# 1. Clone repository
git clone https://github.com/k-zozulia/data_hw_2.git
cd data_hw_2

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Start database services
docker-compose up -d

# 4. Wait for services to be healthy (~30 seconds)
docker-compose ps

# 5. Run ETL pipeline
python pipeline.py

# 6. Run benchmarks
python benchmark/benchmark_databases.py
python benchmark/benchmark_formats.py
python benchmark/benchmark_schemas.py

# 7. Validate data
python validate/data_validator.py
```

### Option B: Manual Setup

```bash
# 1. Install databases manually
# - PostgreSQL 15+
# - MongoDB 7.0+
# - Redis 7.0+

# 2. Update configs/config.py with your connection details

# 3. Follow steps 2, 5-7 from Option A
```

---


## ğŸ’» Usage

### Full ETL Pipeline

Run the complete pipeline (Extract â†’ Transform â†’ Load):

```bash
python pipeline.py
```

This will:
1. Extract data from DummyJSON API
2. Normalize to 3NF
3. Load into PostgreSQL (3NF, Star, Snowflake)
4. Load into MongoDB (denormalized)
5. Cache in Redis

### Individual Modules

#### Extract Only
```bash
python extract/extract.py
```

#### Transform Only
```bash
python transform/transform.py
```

#### Load to Specific Database
```bash
python load/load_postgres.py       # 3NF
python load/load_star_schema.py    # Star Schema
python load/load_snowflake_schema.py  # Snowflake Schema
python load/load_mongo.py          # MongoDB
python load/load_redis.py          # Redis
```

### Run Benchmarks

```bash
# Database performance
python benchmark/benchmark_databases.py

# File format comparison
python benchmark/benchmark_formats.py

# Schema query performance
python benchmark/benchmark_schemas.py
```

### Generate Test Data

```bash
python generate/test_data_generator.py
```

Generates 10,000 test records for performance testing.


### Data Validation

```bash
python validate/data_validator.py
```
Validate data in files and Postgres db.


### Export Fact Tables

```bash
python export/export_fact_tables.py
```

Exports fact tables to Parquet and JSON formats.

## ğŸ“ Project Structure

```
etl-project/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ RESULTS.md                    # Benchmark results & conclusions
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ docker-compose.yml            # Docker services configuration
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py                 # Database & API configuration
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py                 # Centralized logging
â”‚
â”œâ”€â”€ extract/
â”‚   â””â”€â”€ extract.py                # Data extraction from API
â”‚
â”œâ”€â”€ transform/
â”‚   â””â”€â”€ transform.py              # Data normalization (3NF)
â”‚
â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ load_postgres.py          # Load 3NF to PostgreSQL
â”‚   â”œâ”€â”€ load_star_schema.py       # Load Star Schema
â”‚   â”œâ”€â”€ load_snowflake_schema.py  # Load Snowflake Schema
â”‚   â”œâ”€â”€ load_mongo.py             # Load to MongoDB
â”‚   â””â”€â”€ load_redis.py             # Cache in Redis
â”‚
â”œâ”€â”€ handlers/                     # File format handlers
â”‚   â”œâ”€â”€ base_handler.py           # Abstract base class
â”‚   â”œâ”€â”€ csv_handler.py            # CSV operations
â”‚   â”œâ”€â”€ json_handler.py           # JSON/JSONL operations
â”‚   â”œâ”€â”€ avro_handler.py           # Avro with schema inference
â”‚   â””â”€â”€ parquet_handler.py        # Parquet with compression
â”‚
â”œâ”€â”€ benchmark/                    # Performance benchmarks
â”‚   â”œâ”€â”€ benchmark_databases.py    # DB insert/read comparison
â”‚   â”œâ”€â”€ benchmark_formats.py      # File format comparison
â”‚   â””â”€â”€ benchmark_schemas.py      # Schema query performance
â”‚
â”œâ”€â”€ validate/
â”‚   â””â”€â”€ data_validator.py         # Data validation & integrity
â”‚
â”œâ”€â”€ export/
â”‚   â””â”€â”€ export_fact_tables.py     # Export fact tables
â”‚
â”œâ”€â”€ generate/
â”‚   â””â”€â”€ test_data_generator.py    # Generate test data
â”‚
â”‚
â”œâ”€â”€ sql/                          # SQL schemas & queries
â”‚   â”œâ”€â”€ create_tables_3nf.sql
â”‚   â”œâ”€â”€ create_tables_star.sql
â”‚   â”œâ”€â”€ create_tables_snowflake.sql
â”‚   â”œâ”€â”€ queries_3nf.sql
â”‚   â”œâ”€â”€ queries_star.sql
â”‚   â””â”€â”€ queries_snowflake.sql
â”‚
â”œâ”€â”€ results/                      # Benchmark results
â”‚   â”œâ”€â”€ db_benchmark.txt
â”‚   â”œâ”€â”€ format_benchmark.txt
â”‚   â”œâ”€â”€ schema_benchmark.txt
â”‚   â””â”€â”€ pipeline.txt
â”‚
â”œâ”€â”€ data/                         # Data files
â”‚   â”œâ”€â”€ raw/                      # Extracted raw data
â”‚   â”œâ”€â”€ processed/                # Normalized data
â”‚   â”œâ”€â”€ test/                     # Test datasets
â”‚   â””â”€â”€ exports/                  # Exported fact tables
â”‚
â””â”€â”€ pipeline.py                   # Main ETL orchestrator
```

## ğŸ—„ï¸ Database Schemas

### 3NF (Third Normal Form)

Fully normalized relational schema with 11 tables:

**Core Entities:**
- `users` â€” User profiles
- `products` â€” Product catalog
- `orders` â€” Customer orders

**Supporting Tables:**
- `addresses` â€” Physical addresses
- `banks` â€” Payment information
- `companies` â€” Company data
- `categories` â€” Product categories
- `order_items` â€” Order line items
- `reviews` â€” Product reviews
- `product_tags` â€” Product tags
- `product_images` â€” Product images

**ERD Diagram:** 

![erd_3nf.png](diagrams/erd_3nf.png)

### Star Schema

Dimensional model optimized for analytics (5 tables):

**Fact Table:**
- `star_fact_orders` â€” Transaction facts with metrics

**Dimension Tables:**
- `star_dim_users` â€” User dimension
- `star_dim_products` â€” Product dimension (denormalized)
- `star_dim_date` â€” Date dimension (pre-generated)
- `star_dim_location` â€” Geographic dimension

**ERD Diagram:** 

![erd_star.png](diagrams/erd_star.png)

**Advantages:**
- Fast aggregations (fewer JOINs)
- Simple query structure
- Optimized for BI tools

### Snowflake Schema

Normalized dimensional model (9 tables):

**Fact Table:**
- `snow_fact_orders`

**Main Dimensions:**
- `snow_dim_users` (references sub-dimensions)
- `snow_dim_products` (references sub-dimensions)
- `snow_dim_date`

**Sub-Dimensions:**
- `snow_dim_user_roles`
- `snow_dim_categories`
- `snow_dim_brands`
- `snow_dim_states`
- `snow_dim_cities`

**ERD Diagram:**

![erd_snowflake.png](diagrams/erd_snowflake.png)

**Advantages:**
- Reduced redundancy
- Easier maintenance
- Better for hierarchical data

## ğŸ“Š Performance Results

### Database Comparison

| Operation | PostgreSQL | MongoDB | Redis | Winner |
|-----------|------------|---------|-------|--------|
| Insert 10k records | 0.127s | 0.088s | 0.081s | ğŸ† Redis |
| Read single item | 0.14ms | 0.18ms | 0.10ms | ğŸ† Redis |
| Read filtered set | 0.29ms | 0.43ms | N/A | ğŸ† PostgreSQL |

**Batch insert is 9.9x faster than single insert in PostgreSQL.**

### File Format Comparison

| Format | Size (MB) | Read (s) | Write (s) | Best For |
|--------|-----------|----------|-----------|----------|
| CSV | 4.41 | 0.231 | 0.078 | Human-readable |
| JSON | 9.99 | **0.029** | 0.125 | APIs, web |
| Avro | 4.26 | 0.074 | **0.068** | Streaming |
| Parquet | **0.48** | 0.113 | 0.260 | Analytics |

**Parquet achieves 10.4:1 compression ratio.**

### Schema Query Performance

| Schema | Avg Query Time | Performance vs 3NF |
|--------|----------------|---------------------|
| 3NF | 0.0013s | Baseline |
| Star | **0.0008s** | **38% faster** ğŸ† |
| Snowflake | 0.0009s | 31% faster |

**Star Schema is fastest for OLAP queries due to denormalized dimensions.**

*Full results and analysis in [RESULTS.md](RESULTS.md)*

## ğŸ› ï¸ Technologies

### Backend
- **Python 3.11+** â€” Main programming language
- **psycopg2** â€” PostgreSQL adapter
- **pymongo** â€” MongoDB driver
- **redis-py** â€” Redis client

### Data Processing
- **pandas** â€” Data manipulation
- **pyarrow** â€” Parquet support
- **fastavro** â€” Avro serialization
- **requests** â€” API calls

### Databases
- **PostgreSQL 15** â€” Relational database
- **MongoDB 7.0** â€” Document database
- **Redis 7.0** â€” In-memory cache

### DevOps
- **Docker & Docker Compose** â€” Containerization
- **pgAdmin 4** â€” PostgreSQL GUI
- **Redis Commander** â€” Redis GUI